import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

#Analizamos los datos de las letras para decidir una arquitectura de red
#En este caso, analizamos la primer componente principal y la matriz de covarianza para encontrar cuantas (y cuales) variables contienen mayor información, esto nos podría dar una idea de cuantos nodos deben tener las capas ocultas

#letras
#letters = np.empty(7)
letters=np.array([0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00])#0x20: space
letters=np.vstack((letters,[0x04,0x04,0x04,0x04,0x04,0x00,0x04]))#0x21: !
letters=np.vstack((letters,[0x09,0x09,0x12,0x00,0x00,0x00,0x00]))#0x22: "
letters=np.vstack((letters,[0x0a,0x0a,0x1f,0x0a,0x1f,0x0a,0x0a]))#0x23: #
letters=np.vstack((letters,[0x04,0x0f,0x14,0x0e,0x05,0x1e,0x04]))#0x24: $
letters=np.vstack((letters,[0x19,0x19,0x02,0x04,0x08,0x13,0x13]))#0x25: %
letters=np.vstack((letters,[0x04,0x0a,0x0a,0x0a,0x15,0x12,0x0d]))#0x26: &
letters=np.vstack((letters,[0x04,0x04,0x08,0x00,0x00,0x00,0x00]))#0x27: '
letters=np.vstack((letters,[0x02,0x04,0x08,0x08,0x08,0x04,0x02]))#0x28: (
letters=np.vstack((letters,[0x08,0x04,0x02,0x02,0x02,0x04,0x08]))#0x29: )
letters=np.vstack((letters,[0x04,0x15,0x0e,0x1f,0x0e,0x15,0x04]))#0x2a: *
letters=np.vstack((letters,[0x00,0x04,0x04,0x1f,0x04,0x04,0x00]))#0x2b: +
letters=np.vstack((letters,[0x00,0x00,0x00,0x00,0x04,0x04,0x08]))#0x2c: ,
letters=np.vstack((letters,[0x00,0x00,0x00,0x1f,0x00,0x00,0x00]))#0x2d: -
letters=np.vstack((letters,[0x00,0x00,0x00,0x00,0x00,0x0c,0x0c]))#0x2e: .
letters=np.vstack((letters,[0x01,0x01,0x02,0x04,0x08,0x10,0x10]))#0x2f: /
letters=np.vstack((letters,[0x0e,0x11,0x13,0x15,0x19,0x11,0x0e]))#0x30: 0
letters=np.vstack((letters,[0x04,0x0c,0x04,0x04,0x04,0x04,0x0e]))#0x31: 1
letters=np.vstack((letters,[0x0e,0x11,0x01,0x02,0x04,0x08,0x1f]))#0x32: 2
letters=np.vstack((letters,[0x0e,0x11,0x01,0x06,0x01,0x11,0x0e]))#0x33: 3
letters=np.vstack((letters,[0x02,0x06,0x0a,0x12,0x1f,0x02,0x02]))#0x34: 4
letters=np.vstack((letters,[0x1f,0x10,0x1e,0x01,0x01,0x11,0x0e]))#0x35: 5
letters=np.vstack((letters,[0x06,0x08,0x10,0x1e,0x11,0x11,0x0e]))#0x36: 6
letters=np.vstack((letters,[0x1f,0x01,0x02,0x04,0x08,0x08,0x08]))#0x37: 7
letters=np.vstack((letters,[0x0e,0x11,0x11,0x0e,0x11,0x11,0x0e]))#0x38: 8
letters=np.vstack((letters,[0x0e,0x11,0x11,0x0f,0x01,0x02,0x0c]))#0x39: 9
letters=np.vstack((letters,[0x00,0x0c,0x0c,0x00,0x0c,0x0c,0x00]))#0x3a: :
letters=np.vstack((letters,[0x00,0x0c,0x0c,0x00,0x0c,0x04,0x08]))#0x3b: ;
letters=np.vstack((letters,[0x02,0x04,0x08,0x10,0x08,0x04,0x02]))#0x3c: <
letters=np.vstack((letters,[0x00,0x00,0x1f,0x00,0x1f,0x00,0x00]))#0x3d: =
letters=np.vstack((letters,[0x08,0x04,0x02,0x01,0x02,0x04,0x08]))#0x3e: >
letters=np.vstack((letters,[0x0e,0x11,0x01,0x02,0x04,0x00,0x04]))#0x3f: ?

#inicializar
sc = StandardScaler()
pca = PCA()
#estandarizar los datos
scaled_letters = sc.fit_transform(letters)

#calcular pca
pca.fit(scaled_letters)

print('Componente Principal')
print(pca.components_[0])

#calcular la matriz de covarianza
cov_matrix = np.cov(scaled_letters.T)
#calcular los autovalores
v, w = np.linalg.eig(cov_matrix)
#la suma de los autovalores
sum_autovalores = sum(v)
#encontrar la proporción de información que da cada variable
print('Proporción de información de cada variable')
print(v / sum_autovalores)

#Resultados:
#-----------

    #Componente Principal
    #[0.41312469 0.52445439 0.2878388  0.11346059 0.20032902 0.48928671 0.42310506]

    #Proporción de información de cada variable
    #[0.35578295 0.24610255 0.14171184 0.09334189 0.03676992 0.07162092 0.05466993]

#Las 3 primeras variables son las que guardan la mayor cantida de información, nos podería indicar que la última capa oculta necesitaría 3 nodos?
